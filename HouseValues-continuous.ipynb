{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaa3b285ce4154a",
   "metadata": {},
   "source": [
    "# Harris County Home Price Estimations"
   ]
  },
  {
   "cell_type": "code",
   "id": "b46db3d349b43b10",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8cbbf874529a3f7",
   "metadata": {},
   "source": [
    "## Building and Real Account Data\n",
    "This has the base features to include for the model. In this file I am only pulling the continuous data. So using the date built, improvement square feet, gross area, base area, land area, perimeter and size index to estimate the assessed_val."
   ]
  },
  {
   "cell_type": "code",
   "id": "cd1a018ec0b04206",
   "metadata": {},
   "source": [
    "con = sqlite3.connect('HouseProtestValues.db')\n",
    "sql_query = '''SELECT  br.acct,\n",
    "                       br.bld_num,\n",
    "                       br.date_erected,\n",
    "                       br.im_sq_ft,\n",
    "                       br.perimeter,\n",
    "                       ra.land_val,\n",
    "                       ra.bld_val,\n",
    "                       ra.assessed_val,\n",
    "                       ra.tot_appr_val\n",
    "            FROM building_res as br\n",
    "            LEFT JOIN real_acct as ra ON br.acct = ra.acct\n",
    "            WHERE br.impr_tp = 1001 AND br.property_use_cd = 'A1' AND br.date_erected > 10;'''\n",
    "\n",
    "base_df = pd.read_sql_query(sql_query, con)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "446e02ab999aedda",
   "metadata": {},
   "source": [
    "## Fixtures Data\n",
    "This has features such as, number of rooms such as bedrooms (RMB), full bath (RMF), half bath (RMH) and total rooms (RMT). This will be merged in a pandas dataframe based on the account number and building number. This data was storred in a table format with multiple accounts and building numbers for each feature, so I created a pivot table with the features as columns."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1a5b53ae054fb70",
   "metadata": {},
   "source": [
    "# Story Height Index: STY\n",
    "# Room: Bedroom: RMB\n",
    "# Room: Full Bath: RMF\n",
    "# Room: Half Bath: RMH\n",
    "# Room: Total: RMT\n",
    "fixtures_sql = \"\"\"SELECT *\n",
    "                  FROM \"fixtures\"\n",
    "                  WHERE type IN ('STY', 'RMB','RMF','RMH','RMT')\n",
    "                \"\"\"\n",
    "fixtures = pd.read_sql_query(fixtures_sql, con)\n",
    "\n",
    "# Pivot table\n",
    "fix_pt = fixtures.pivot_table(index=['acct', 'bld_num'], columns='type', values='units', aggfunc='sum')\n",
    "fix_pt = fix_pt.reset_index()\n",
    "fix_pt.fillna(0, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7a9619b1bad1275",
   "metadata": {},
   "source": [
    "## Merge Base Data with Fixtures Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0ec9a5fb0afe065",
   "metadata": {},
   "source": [
    "data_df = pd.merge(base_df, fix_pt, on=['acct', 'bld_num'], how='left')\n",
    "data_df.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1013b380b998a6e3",
   "metadata": {},
   "source": [
    "## Reduce and sample data_df\n",
    "There are over 1 million different residential houses that have data and that is too many to run the whole set on the model. I will use two techniques to reduce the data set, fist I will use a heuristic where account that has less than 50 square feet of improvements will be removed; these are mostly empty lots that have no livable domiciles. Next I will remove outliers that can skew the data by using the inner quartile range, and then I will have pandas pull a sample of the remaining. \n",
    " * Remove rows where 'im_sq_ft' is less than 50. \n",
    " * IQR: inner quartile range is a technique that is used to remove outliers. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Heuristics\n",
    "# Remove accounts with less than 50 square feet of improvement area\n",
    "data_df = data_df[data_df['im_sq_ft'] > 50]"
   ],
   "id": "d4f167e1f3db5f09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# IQR\n",
    "Q1 = data_df['assessed_val'].quantile(0.25)\n",
    "Q3 = data_df['assessed_val'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_iqr = (Q1 - 1.5 * IQR)\n",
    "upper_iqr = (Q3 + 1.5 * IQR)\n",
    "print(f\"Lower IQR: {lower_iqr} | Upper IQR: {upper_iqr}\")"
   ],
   "id": "f18ebf3cc025e5a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter data_df to values between Lower IQR and Upper IQR\n",
    "reduced_df = data_df[data_df['assessed_val'] <= upper_iqr]"
   ],
   "id": "bf7c50c61f9273cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8613777476d40d52",
   "metadata": {},
   "source": [
    "sample_df = reduced_df.sample(n=5000, random_state=42)\n",
    "\n",
    "x = sample_df[['date_erected', 'im_sq_ft', 'perimeter', 'RMB', 'RMF', 'RMH', 'RMT', 'STY']]\n",
    "y = sample_df['assessed_val']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7e48aab4fcd6150",
   "metadata": {},
   "source": [
    "print(f\"All Data{sample_df.shape} | x {x.shape} | y {y.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_df.describe()",
   "id": "e51b22c691edf8f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bb52e2e31131748",
   "metadata": {},
   "source": [
    "# Free up memory\n",
    "base_df = None\n",
    "fix_pt = None\n",
    "fixtures = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e6f3be4d4510cd1",
   "metadata": {},
   "source": [
    "# Histogram of assessed values\n",
    "sns.histplot(data=sample_df, x=\"assessed_val\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d986d7b18d308750",
   "metadata": {},
   "source": [
    "# Histogram with log transformation\n",
    "sns.histplot(data=sample_df, x=\"assessed_val\", log_scale=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "transformed_data, lambda_value = boxcox(sample_df['assessed_val'])\n",
    "sample_df['transformed_data'] = transformed_data\n",
    "sns.histplot(data=sample_df, x=\"transformed_data\")"
   ],
   "id": "888571004cebc6c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test normality with Shapiro-Wilk Test\n",
    "The Shapiro-Wilk test evaluates a data set and quantifies how likely it is that the data was sampled from a Gaussian distribution. This is believed to be a reliable test for naormality if the dataset is not too large, i.e. under 5,000.\n",
    "\n"
   ],
   "id": "119261578be09e3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stat, p = shapiro(np.log(sample_df['assessed_val']))\n",
    "print(f\"H-null: the distribution is normal\\nH-alternative: the distribution is not normal\")\n",
    "print(f'Statistic: {stat} | p: {p}')\n",
    "alpha = 0.05  # General alpha for 95% confidence\n",
    "if p > alpha:\n",
    "    print(f'Sample looks normally distributed (fail to reject H-null).')\n",
    "else:\n",
    "    print('Sample does not look normally distributed (reject H-null in favor of H-alternative).')"
   ],
   "id": "3cae9cc8a764969c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test if box cox transformation normalizes the data\n",
    "stat, p = shapiro(sample_df['transformed_data'])\n",
    "print(f\"H-null: the distribution is normal\\nH-alternative: the distribution is not normal\")\n",
    "print(f'Statistic: {stat} | p: {p}')\n",
    "alpha = 0.05  # General alpha for 95% confidence\n",
    "if p > alpha:\n",
    "    print(f'Sample looks normally distributed (fail to reject H-null).')\n",
    "else:\n",
    "    print('Sample does not look normally distributed (reject H-null in favor of H-alternative).')"
   ],
   "id": "ef5d6c996c1e26b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67c510dcfae1eb55",
   "metadata": {},
   "source": [
    "corr_matrix = sample_df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(data=corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc5de848c006de1b",
   "metadata": {},
   "source": [
    "sns.pairplot(data=sample_df, vars=['assessed_val', 'tot_appr_val', 'date_erected', 'im_sq_ft',\n",
    "                                   'perimeter', 'RMB', 'RMF', 'RMH', 'RMT', 'STY'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da9928e4011cfbd9",
   "metadata": {},
   "source": [
    "## Train, Test, Split!\n",
    "The training and testing sets get split, but I will need to see some examples to see if there are indexes on the y's"
   ]
  },
  {
   "cell_type": "code",
   "id": "8cd1b1deae616863",
   "metadata": {},
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Ranking",
   "id": "bd9629808dc86e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rfc = RandomForestClassifier()",
   "id": "5025b2c9fa11d36a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rfc.fit(x_train, y_train)\n",
    "importance = rfc.feature_importances_"
   ],
   "id": "c7b83374d69467c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_importance = pd.Series(importance, index=x_train.columns)\n",
    "print(feature_importance.sort_values(ascending=False))"
   ],
   "id": "a4c8130d57941339",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63f3e82b1b9cef4b",
   "metadata": {},
   "source": "etc = ExtraTreesRegressor(random_state=42)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79c98ae8b1e0f54c",
   "metadata": {},
   "source": [
    "etc.fit(x_train, y_train)\n",
    "etc.feature_importances_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4488ad5c2a098fe8",
   "metadata": {},
   "source": "cross_val_score(etc, x_train, y_train, cv=5, n_jobs=-1).mean()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "372806e1ddda94d5",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 750, 800, 1000, 5000],\n",
    "    'min_samples_leaf': [0.25, 0.5, 1, 3, 4],\n",
    "    'max_features': [\"sqrt\", \"log2\", 5, 7, 10, 15],\n",
    "    'criterion': ['absolute_error', 'friedman_mse']\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4a73c266f8f8b05",
   "metadata": {},
   "source": [
    "# n_jobs will determine the amount of parallel process and how much memory will be used. -1 will use ALL cores, set to 50% of virtual cores to be safe and not use all the memory.\n",
    "etc2 = GridSearchCV(etc, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bcb64c5198da257",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "etc2.fit(x_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66ea07fcb01336fb",
   "metadata": {},
   "source": [
    "etc2.best_params_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c35939d6dc020571",
   "metadata": {},
   "source": [
    "etc2.best_score_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e9dad8d63135bd2",
   "metadata": {},
   "source": [
    "y_pred = etc2.predict(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf57eb7b1b117224",
   "metadata": {},
   "source": [
    "r2_score(y_test, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(etc2, 'etc.pkl')\n",
    "\n",
    "# load\n",
    "# joblib.load('etc.pkl')"
   ],
   "id": "e259e285d8ad3268",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74931a0e22b1f8d0",
   "metadata": {},
   "source": [
    "# Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c5a24b3d994fd1",
   "metadata": {},
   "source": [
    "actual = y_test.to_list()\n",
    "predicted = etc2.predict(x_test)\n",
    "\n",
    "act_pred_df = pd.DataFrame({'actual': actual, \"predicted\": predicted, })\n",
    "act_pred_df['residuals'] = act_pred_df['actual'] - act_pred_df['predicted']\n",
    "act_pred_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b4a0c540a52689a",
   "metadata": {},
   "source": [
    "sns.scatterplot(data=act_pred_df, x='actual', y='predicted')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8329c69a53ca90d",
   "metadata": {},
   "source": [
    "sns.scatterplot(data=act_pred_df, x='actual', y='residuals')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9540e1f56b277a9",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "The assessed values are not normally distributed, but are skewed right. They will be log transformed and the predicted values will be expectationaled back to assess the residuals."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_df['log(values)'] = np.log(sample_df['tot_appr_val'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ],
   "id": "b921896ad69fb561",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e173d1734bca0f0b",
   "metadata": {},
   "source": [
    "gbr = GradientBoostingRegressor()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd085d05138f02a5",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.0005, 0.002],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [3, 4, 6],\n",
    "    'min_samples_leaf': [2, 3, 6],\n",
    "    'criterion': ['friedman_mse'],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38b3fb7c7a27a98d",
   "metadata": {},
   "source": [
    "gbr_cv = GridSearchCV(gbr, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30a8e321f33b61f1",
   "metadata": {},
   "source": [
    "gbr_cv.fit(x_train, y_train)\n",
    "gbr_pred = gbr_cv.predict(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "794072b957fcf34b",
   "metadata": {},
   "source": [
    "gbr_mae = mean_absolute_error(y_test, gbr_pred)\n",
    "gbr_mse = mean_squared_error(y_test, gbr_pred)\n",
    "gbr_r2 = r2_score(y_test, gbr_pred)\n",
    "print(f\"MAE: {gbr_mae}\\nMSE: {gbr_mse}\\nR Squared: {gbr_r2}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94625c1b837f55cd",
   "metadata": {},
   "source": [
    "gbr_cv.best_params_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b56a3054adae1ae2",
   "metadata": {},
   "source": [
    "gbr_cv.best_score_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86b77f49299ff09",
   "metadata": {},
   "source": [
    "joblib.dump(gbr_cv, 'gbr.pkl')\n",
    "\n",
    "# load\n",
    "# joblib.load('gbr.pkl')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c397df70e0aae59",
   "metadata": {},
   "source": [
    "gbr_residual_df = pd.DataFrame({'actual': actual, \"predicted\": gbr_pred, })\n",
    "gbr_residual_df['residuals'] = gbr_residual_df['actual'] - gbr_residual_df['predicted']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46fa0bdab40f4abf",
   "metadata": {},
   "source": [
    "sns.regplot(gbr_residual_df, x='actual', y='predicted')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71f8f3e25c4ea4cd",
   "metadata": {},
   "source": [
    "sns.regplot(gbr_residual_df, x='actual', y='residuals')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9541d4541f72d6af",
   "metadata": {},
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a8a2136ffda2877",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(gbr_residual_df['actual'], gbr_residual_df['predicted'], s=60, alpha=0.7, edgecolors='k')\n",
    "b, a = np.polyfit(gbr_residual_df['actual'], gbr_residual_df['predicted'], 1)\n",
    "ax.plot(gbr_residual_df['actual'], b * gbr_residual_df['actual'] + a)\n",
    "ax.annotate(f\"R-Squared = {gbr_r2}\", (0, 1))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bf395b2074add6ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6a53f6b9298f9876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8dd150263ab7ddca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "14aea4c618274d73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1c4a6c8c7d91883d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
